# Caching

Whenever we build infrastructure, we want to minimize latency of course has with most things there are tradeoffs, but in general we always aim to minimize latency as much as possible. And one of the best ways to mininmize latency is `Caching`, which can also happen at many point in a system such a the client, server, database, and even at the cpu, has there are so many place we cache information.

- Caching is used to speed up a system, to reduce or improve the latency of a system. So any computational system that might take a lot of time or that will perform multiple time, we want to cache that result so that we can avoid having to wait for the result again, and caching is able to do that by storing somewhere it can be easily accessible and we don't have to make external call or retrieve that data from the original place where that data was stored.
- So we are basically storing that data in a more conviennt location, and caching dis used everywhere in software. for example we use caching to impove the time complexity of our algorithms in other to speed up our algorithms, we use caching at the hardware level, such the cpu cache which may not even know thus existing, yes the cpu cache things for example, recently executed instructions or recently loaded data that can be accessed much faster with a much lower latency because of the cpu cache, expecailly because many algorithm touch the same data and memory multiple time this make it faster with a cpu cache. And other hardware uses cache such as the harddrive has a cache which is called dick buffer or disk cache whcih is basically the embedded memory in your hard-disk drive which acts as a temporary memory space for your hard drive writes to permanent storage, that is simply a cache.
- At the software level, there is caching everywhere, there is caching at the client level, server level, database level, caching in the web browser, caches exist everywhere. because it make sense because they save time (speed up the system), they save you from performing operation multiple time needlessly (miximize work), of course as engineers we like to optimize everything, caches are optimal.

Caching usually have one of two such as:

- Speed up system.
- Or minimize work.

Let give some concrete examples.
![alt cachingexp](/images/cachingexp.png)
Here we have a client, the server and a database. understand the simple system where we have the client that makes a request to a server for some data, that server in turn ask the database for this data or to change this data, and the database responds back to this seever and the server sends that data back to the client. Here as we can see we have four parts of communication and alot this happen over a network which is probably not ideal because its not neccessary in all cases, expecially if it is a data which is static or we are requesting static content that doesn't change or that is immutable (which the user can't even change), then why not cache it, and remove some extra step.

- We always want to remove as many unneccessary steps as possible, so we could for example have a cache at the client level which would remove the unneccessary steps of making extra network request of the client sending a request to server, the server  sending response back. We don't those extra network request.
- So having a cache at the client level would help that, a good example for this is instagram, If you've ever scroll through instagram when you don't have wifi or data, you've probably noticed that your pictures or videos still load. And that is because instragem caches those that are the are at the client levels, so that means that you don't need to be connected too a internet or data in other to see those data. Also they might have likely cache at other levels such as the server in other to not have to hit the database again. This is always common because maybe you need the client to always interact with the server but maybe the server doesn't always need to go to the database to retrieve data. Maybe it only needs to go to the database onces and then we can have some form of cache, here at the server level. We also have caching at the database level which we discuss [database section](/database.md).
  - But to give an example of why we would want to cache at the server level. If alot of people are requesting a popular user profile, we wouldn't want to make a million calls a day to the database, when we can just save our most recent pictures and the user profile on the server and more efficiently distributed to the client that are making that request. Because of course in the real-world systems and infrastructure is much more complex than just one client and one server, we typically have multiple servers that servers the same content for alot of reasons like uptime, avilability, fourth tolerance, load balancing etc.
  - But this really complicate things when we talk about caching, and things gets a bit tricker when we are dealing with mutable content, that is content that users can edit which is not static. imagine we capture your user profile, unless you go and edit it that would usually be the same thing. There is no problem to cache information like that on the client level and not make any network request, thats why you'll notice if you are not connected to Wifi or data you can still see your own instagram profile thats static data and doesn't really need to change, that usually cached at the client level.
  - However, if we cache posts or comments this is where this might become a problem because for example, if you've ever try to reply to an instagram comment, then in the meantime it saids you can't reply, because this comment has been deleted, well that problem would happen alot more if we cached comments and data like that which are mutable (always changing). So when it comes to static contain it really easy, because we are usually only reading the content, and not writing it. But what if we have a mutable content like posting to instagram, writing comments we wouldn't ant to show stale caches or old comments that have been deleted or edited.
  - Thats why we have the process of caching validation, which is essentially just clearing anything that is invalid, so replacing and removing stale entries.
  ![alt cachingexp2](/images/cachingexp2.png)

## Here are some good rules of thumb to consider when using caching

- If you only going to have one thing, reading or writing data then you can use cacching.
- If you don't care about the staleness of data.
- If you able to properly invalidate or get rid of stale data, use caching.
- And if you have a distributed system, you absolutely can or should use a cache but that is when it becomes a bit more complicated, and we have to get to this more complex caching mechanism, because we can't store infinite data in the cache and plus we might have stale data which we need to get rid of that data and replace it with new fresh data like for example an updated comment or edited post, so keepign the cache in sync with the data source is one of the hardest problems for engineers, and deciding how to do that has been an engineering challenge for years.

### Here are some of the few caching types you should know

- Spatial cache: it pulls nearby located data, so for example data that is located within the visinity of the data that you are trying to get from the cache at a certain point in time.
- Temporal cache: The idea of temporal caching is that if you are using a piece of data alot, then you are actually going to use it in the feature, so you keep it fresh.
- Distributed cache: It the combination of both time and spatial, and the trick here is that you actually need to the cache in sync with the main datastore, one of the most popular distributed caching mechanism is redis, so you would have a central cache for example redis where we want to read a piece of data from your cache if your cache is empty you go to the disk and you pull that data or info and you populate the cache with that data, so that the next time you want to read that data, you ask redis if it has the data, and redis would redis back with the data or not. the issues with this approach is that the data in redis might be stale. So to avoid the data being stale, ensuring it upto date we use the following method:
  - Writethrough cache:
  - Writeback cache:
